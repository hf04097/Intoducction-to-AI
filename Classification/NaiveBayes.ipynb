{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(fileName):\n",
    "    dataset = pd.read_table(fileName, header=0, sep=\",\", encoding=\"unicode_escape\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess creates the term frequency matrix for the review data set\n",
    "def preprocess(data):\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    data = count_vectorizer.fit_transform(data)\n",
    "    #tfidf_data = TfidfTransformer(use_idf=False).fit_transform(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveClassifier:\n",
    "     def __init__(self):\n",
    "            self.target_labels = None\n",
    "            self.target_labels_counts = 0\n",
    "            self.prob_dict = dict()\n",
    "            self.prob_mat = None\n",
    "            \n",
    "    \n",
    "     def Naive_learn_model(self,data,target):\n",
    "  \n",
    "        self.target_labels = target.value_counts().index    #what categories are there\n",
    "        self.target_labels_counts = target.value_counts().array   #how many categories\n",
    "        vector_Y = target.array\n",
    "        total_Y = vector_Y.shape[0]\n",
    "        matrix_X = data.toarray()\n",
    "        self.prob_mat = np.zeros((len(self.target_labels),matrix_X.shape[1],2))    #3d array to store conditional prob\n",
    "        t_matrix_X = matrix_X.T\n",
    "\n",
    "        for i in range(len(self.target_labels)):\n",
    "            self.prob_dict[self.target_labels[i]]= ((self.target_labels_counts[i])+1)/((total_Y)+len(self.target_labels))   #probability of labels\n",
    "\n",
    "        for i in range(len(self.target_labels)):\n",
    "            row_number =0\n",
    "            for row in t_matrix_X:\n",
    "                prob1 =0\n",
    "                prob0 = 0\n",
    "                target_index = np.where(vector_Y == self.target_labels[i])[0]\n",
    "                vals, counts = np.unique(row[target_index],return_counts = True)\n",
    "                self.prob_mat[i,row_number,0] = (counts[0]+1)/((len(target_index))+2)   # adding 1 for laplacian\n",
    "                self.prob_mat[i,row_number,1] = (np.sum(counts[1:])+1)/((len(target_index))+2)\n",
    "                row_number+=1\n",
    "    \n",
    "     def Naive_classify(self,testdata):\n",
    "            x_test = testdata.toarray()\n",
    "            category_predicted = []\n",
    "            prob_list = np.zeros(len(self.target_labels))\n",
    "            \n",
    "            for row in range(x_test.shape[0]):\n",
    "                for k in range(len(self.target_labels_counts)):\n",
    "                    prob_test = 1\n",
    "                    for ind, i in enumerate(x_test[row]):\n",
    "                        if i ==0:\n",
    "                            prob_test = prob_test* self.prob_mat[k,ind,0]\n",
    "                        else:\n",
    "                            prob_test = prob_test* self.prob_mat[k,ind,1]\n",
    "                    prob_list[k] = prob_test * self.prob_dict[self.target_labels[k]]\n",
    "                    \n",
    "                max_prob_index = np.where(prob_list == np.amax(prob_list))\n",
    "                category_predicted.append(self.target_labels[max_prob_index[0][0]])\n",
    "            return category_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_model(data,target):\n",
    "    classifier = NaiveClassifier()\n",
    "    classifier.Naive_learn_model(data,target)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(classifier, testdata):\n",
    "    \n",
    "    predicted_val= classifier.Naive_classify(testdata)\n",
    "    return predicted_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Confusion_mat(actual,predicted):\n",
    "    target_labels = np.unique(actual).tolist()\n",
    "    confusion_mat = np.zeros((len(target_labels),len(target_labels)))\n",
    "    actual = np.asarray(actual)\n",
    "    \n",
    "    for i in range(len(actual)):\n",
    "        actual_index = target_labels.index(actual[i])\n",
    "        pred_index = target_labels.index(predicted[i])\n",
    "        confusion_mat[actual_index,pred_index]+=1\n",
    "    return confusion_mat\n",
    "\n",
    "def individual_confusion_mat(actual,predicted):\n",
    "    \n",
    "    target_labels = np.unique(actual)\n",
    "    confusion_mat = Confusion_mat(actual,predicted)\n",
    "    individual_prob_mat =[]\n",
    "    \n",
    "    for i in range(len(target_labels)):\n",
    "        individual_prob_mat_a = confusion_mat[i,i]\n",
    "#         individual_prob_mat_b = np.sum(confusion_mat[i,:])- individual_prob_mat_a\n",
    "#         individual_prob_mat_c = np.sum(confusion_mat[:,i])- individual_prob_mat_a\n",
    "        individual_prob_mat_b = confusion_mat[i,:][confusion_mat[i,:]!= individual_prob_mat_a].sum()\n",
    "        individual_prob_mat_c = confusion_mat[:,i][confusion_mat[:,i]!= individual_prob_mat_a].sum()\n",
    "        individual_prob_mat_d = np.sum(confusion_mat) - individual_prob_mat_a - individual_prob_mat_b - individual_prob_mat_c\n",
    "        individual_prob_mat.append((individual_prob_mat_a,individual_prob_mat_b,individual_prob_mat_c,individual_prob_mat_d))\n",
    "    return individual_prob_mat\n",
    "    \n",
    "def precision_recall_fmeasure(actual,predicted):\n",
    "    individual_prob_mat = individual_confusion_mat(actual,predicted)\n",
    "    precision_list= []\n",
    "    recall_list= []\n",
    "    fmeasure_list =[]\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    fmeasure = 0\n",
    "    actual_np = np.asarray(actual)\n",
    "    unique_elements, counts_elements = np.unique(actual_np, return_counts=True)\n",
    "    \n",
    "    for i in individual_prob_mat:\n",
    "        if((i[0]+i[2])==0):\n",
    "            precision_list.append(0)\n",
    "        else:\n",
    "            precision_list.append((i[0])/(i[0]+i[2]))\n",
    "        \n",
    "        if((i[0]+i[1]) == 0):\n",
    "            recall_list.append(0)    \n",
    "        else:\n",
    "            recall_list.append((i[0])/(i[0]+i[1]))\n",
    "            \n",
    "        if((2*i[0]+i[1]+i[2]) == 0):\n",
    "            fmeasure_list.append(0)\n",
    "        else:\n",
    "            fmeasure_list.append((2*i[0])/(2*i[0]+i[1]+i[2]))\n",
    "    \n",
    "    for i in range(len(precision_list)):\n",
    "        precision+= (precision_list[i]*counts_elements[i])\n",
    "    \n",
    "    for i in range(len(recall_list)):\n",
    "        recall+= recall_list[i]*counts_elements[i]\n",
    "    \n",
    "    for i in range(len(fmeasure_list)):\n",
    "        fmeasure+= fmeasure_list[i]*counts_elements[i]\n",
    "    \n",
    "    return (precision/(actual_np.shape[0])),(recall/(actual_np.shape[0])),(fmeasure/(actual_np.shape[0]))\n",
    "    \n",
    "\n",
    "def evaluate(actual_class, predicted_class):\n",
    "    p_r_f = precision_recall_fmeasure(actual_class,predicted_class)\n",
    "    accuracy = accuracy_score(actual_class, predicted_class)\n",
    "    \n",
    "    print(\"The precision score is :\",p_r_f[0])\n",
    "    print(\"The recall score is :\",p_r_f[1])\n",
    "    print(\"The f_measure score is :\",p_r_f[2])\n",
    "    print(\"The accuracy score is :\",accuracy)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data.....\n",
      "preprocessing data.....\n",
      "Learning model.....\n"
     ]
    }
   ],
   "source": [
    "features = [\"SUMMARY\", \"categories\", \"sub_categories\"]\n",
    "\n",
    "print(\"Loading data.....\")\n",
    "dataset = load_file(\"TextClassification_Data.csv\")\n",
    "data,target = dataset[features[0]].fillna(\" \"), dataset[features[1]]\n",
    "\n",
    "print(\"preprocessing data.....\")\n",
    "word_vectors = preprocess(data)\n",
    "    \n",
    "trainingX,testX,trainingY,testY = train_test_split(word_vectors,target,test_size=0.4,random_state=43)\n",
    "#print(type(trainingY))\n",
    "#print(trainingY.value_counts().array)\n",
    "\n",
    "print(\"Learning model.....\")\n",
    "model = learn_model(trainingX,trainingY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying test data......\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifying test data......\")      \n",
    "predictedY = classify(model, testX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating results.....\n",
      "The precision score is : 0.7338093072406146\n",
      "The recall score is : 0.7165687426556991\n",
      "The f_measure score is : 0.7173412550457267\n",
      "The accuracy score is : 0.7165687426556991\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating results.....\")\n",
    "evaluate(testY,predictedY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
